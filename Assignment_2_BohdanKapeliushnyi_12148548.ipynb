{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 2: Gaussian Classifier, Bias-Variance Decomposition, Evaluation Measures </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use\n",
    "only. Any reproduction of this material, no matter whether as a\n",
    "whole or in parts, no matter whether in printed or in electronic\n",
    "form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Automatic Testing Guidelines</h2>\n",
    "\n",
    "Automatic unittesting requires you, as a student, to submit a notebook which contains strictly defined objects.\n",
    "Strictness of definition consists of unified shapes, dtypes, variable names, and more.\n",
    "\n",
    "Within the notebook, we provide detailed instructions which you should follow in order to maximize your final grade. Please keep in mind:\n",
    "\n",
    "* Don't add any cells but use the ones provided by us. You may notice that most cells are tagged such that the unittest routine can recognise them.\n",
    "\n",
    "* We highly recommend you to develop your code within the provided cells. You can implement helper functions where needed unless you put them in the same cell they are actually called. Always make sure that implemented functions have the correct output and given variables contain the correct data type. Don't import any other packages than listed in the cell with the \"imports\" tag.\n",
    "\n",
    "* Never use variables you defined in another cell in your functions directly; always pass them to the function as a parameter. In the unittest they won't be available either.\n",
    "\n",
    "*Good luck! :)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: Gaussian classifier: visualization & parameter estimation (10 points)</h2>\n",
    "\n",
    "The goal of this task is to explore the given (artificial) data before diving into the classification function. To do this, we will use `matplotlib` to plot the data set and `numpy` to estimate the means & covariance matrices of the classes as well as the probability of encountering a positive/negative example.\n",
    "\n",
    "* **Task 1.1**: Visualize the data stored in `normal.csv` with two different colors using a scatter plot and store it in the given variable. Always label the axes of all your plots. We also suggest to make a plot legend indicating which color belongs to which label.\n",
    "* **Task 1.2**: We assume that the data is distributed according to a two-dimensional (bivariate) normal distribution:\n",
    "    - Write a function that estimates the mean and covariance matrix for the entire dataset, the means and covariance matrices for each class, and the probabilities $p(y=+1)$ and $p(y=-1)$.\n",
    "    - Return a tuple containing the results (the resulting list should be of length 8). The datatype for `meanX`, `covX`, `meanXpos`, `covXpos`, `meanXneg`, and `covXneg` should be a numpy array, for $p(y=+1)$ and $p(y=-1)$ it should be float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">1.1. Code & question (4 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "data"
    ]
   },
   "outputs": [],
   "source": [
    "# read data, split into X (features) and y (labels)\n",
    "Z = np.genfromtxt('normal.csv', delimiter=',')\n",
    "X, y = Z[:,:-1], Z[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code1.1"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualize the data with a scatter plot\n",
    "## your code goes here ↓↓↓\n",
    "def scatter_plot(X, y):\n",
    "    \"\"\"creates a scatter-plot for the dataset X with labels y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        data\n",
    "    y : np.ndarray\n",
    "        labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Figure\n",
    "        a matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig1 = plt.figure(figsize=(8,5))\n",
    "    # your code goes here ↓↓↓\n",
    "    \n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "fig = scatter_plot(X, y)\n",
    "assert isinstance(fig, Figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "***Answer the following yes/no questions concerning the distribution of the data:***\n",
    "\n",
    "a11_) Would a linear regression method be an optimal choice for this task?<br>\n",
    "b11_) Would a linear classifier achieve a better performance than 25% misclassification?<br>\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question.<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "q1.1"
    ]
   },
   "outputs": [],
   "source": [
    "# examples for you\n",
    "example_of_true_variable = True\n",
    "example_of_false_variable = False\n",
    "\n",
    "# your answers go here ↓↓↓\n",
    "a11_ = None\n",
    "b11_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">1.2. Code (6 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code1.2"
    ]
   },
   "outputs": [],
   "source": [
    "def est_mean_cov(X_,y_):\n",
    "    \"\"\"\n",
    "    Function that estimates the means and covariance matrices from the given data as well \n",
    "    as the probability to encounter a positive/negative example respectively\n",
    "    @param X_, np ndarray, data matrix\n",
    "    @param y_, np ndarray, data vector\n",
    "    Returns\n",
    "    covX, covXpos, covXneg: covariance matrices for entire dataset, positive samples, negative samples\n",
    "    meanX, meanXpos, meanXneg: means for entire dataset, positive samples, negative samples\n",
    "    p_ypos, p_yneg: probabilites p(y=+1), p(y=-1)\n",
    "    \"\"\"\n",
    "    #your code goes here ↓↓↓\n",
    "    \n",
    "    return(meanX, covX, meanXpos, covXpos, meanXneg, covXneg, p_ypos, p_yneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "meanX, covX, meanXpos, covXpos, meanXneg, covXneg, p_ypos, p_yneg = est_mean_cov(X,y)\n",
    "\n",
    "# print corresponding values\n",
    "\n",
    "print(\"Entire dataset:\\n\")\n",
    "print(\"Mean = \", meanX, \"\\n\")\n",
    "print(\"Covariance:\")\n",
    "print(pd.DataFrame(covX,columns=[\"x1\",\"x2\"],index=[\"x1\",\"x2\"]),\"\\n\\n\")\n",
    "\n",
    "print(\"Positive class:\\n\")\n",
    "print(\"Mean = \", meanXpos, \"\\n\")\n",
    "print(\"Covariance:\")\n",
    "print(pd.DataFrame(covXpos,columns=[\"x1\",\"x2\"],index=[\"x1\",\"x2\"]),\"\\n\")\n",
    "print(\"p(y=+1) =\", p_ypos, \"\\n\\n\")\n",
    "\n",
    "print(\"Negative class:\\n\")\n",
    "print(\"Mean =\", meanXneg, \"\\n\")\n",
    "print(\"Covariance:\")\n",
    "print(pd.DataFrame(covXneg,columns=[\"x1\",\"x2\"],index=[\"x1\",\"x2\"]),\"\\n\")\n",
    "print(\"p(y=-1) =\", p_yneg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2: Gaussian classifier: compute classifier & visualization (20 points)</h2>\n",
    "\n",
    "Now that we got a good idea of the data, we want to implement a classifier and visualize it.\n",
    "\n",
    "- **Task 2.1**: Compute an optimal classification function $g$ in `calc_func_g()` (see slide 19/92 \"Explicit example: Gaussian classifier: Part 2\" from lecture Unit2.pdf). To do this, you should:\n",
    "    - Calculate the values of the corresponding parameters $\\mathbf{A}$, $\\mathbf{b}$ and $c$ in the provided functions.\n",
    "    - Store the results in the given parameters **par_A** (np.array), **par_b** (np.array), **par_c** (float), and **func_g** (np.array of shape [500,500]). The $500\\times500$ grid (within coordinates $[-11,11]\\times[-11,11]$) leads to a nice plot.\n",
    "    - Print the values of $\\mathbf{A}$, $\\mathbf{b}$ and $c$ that you have calculated with their respective shapes.\n",
    "    - Note: You can reuse items from the previous task.\n",
    "\n",
    "* **Task 2.2**: Visualize the classification function and the original data samples from Task 1.1. in **one** two-dimensional plot. I.e., the plot should show the data samples -- blue for positive and orange for negative -- and also show the classification function (500x500 grid points) in the corresponding (or similar) colors. For the grid points: use alpha=0.05 for opaquness and s=1 for the marker size. The decision boundary will become visible as the line separating the two classification territories. Again: Label the axes and plot a legend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">2.1 Code (10 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code2.1.1"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_par_A(meanXpos, covXpos, meanXneg, covXneg, p_ypos, p_yneg):\n",
    "    \"\"\"\n",
    "    This function should contain the calculations for the respective parameter and return the result.\n",
    "    @param meanXpos, np ndarray, mean of positive examples\n",
    "    @param covXpos, np ndarray, covariance matrix of positive examples\n",
    "    @param meanXneg, np ndarray, mean of negative examples\n",
    "    @param covXneg, np ndarray, covariance matrix of negative examples    \n",
    "    @param p_ypos, float, probability of encountering a positive example\n",
    "    @param p_yneg, float, probability of encountering a negative example\n",
    "    returns np.ndarray\n",
    "    \"\"\"\n",
    "    #your code goes here ↓↓↓\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code2.1.2"
    ]
   },
   "outputs": [],
   "source": [
    "def calc_par_b(meanXpos,covXpos,meanXneg,covXneg,p_ypos,p_yneg):\n",
    "    \"\"\"\n",
    "    This function should contain the calculations for the respective parameter and return the result.\n",
    "    @param meanXpos, np ndarray, mean of positive examples\n",
    "    @param covXpos, np ndarray, covariance matrix of positive examples\n",
    "    @param meanXneg, np ndarray, mean of negative examples\n",
    "    @param covXneg, np ndarray, covariance matrix of negative examples\n",
    "    @param p_ypos, float, probability of encountering a positive example\n",
    "    @param p_yneg, float, probability of encountering a negative example\n",
    "    returns np.ndarray\n",
    "    \"\"\"\n",
    "    #your code goes here ↓↓↓\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code2.1.3"
    ]
   },
   "outputs": [],
   "source": [
    "def calc_par_c(meanXpos,covXpos,meanXneg,covXneg,p_ypos,p_yneg):\n",
    "    \"\"\"\n",
    "    This function should contain the calculations for the respective parameter and return the result.\n",
    "    @param meanXpos, np ndarray, mean of positive examples\n",
    "    @param covXpos, np ndarray, covariance matrix of positive examples\n",
    "    @param meanXneg, np ndarray, mean of negative examples\n",
    "    @param covXneg, np ndarray, covariance matrix of negative examples\n",
    "    @param p_ypos, float, probability of encountering a positive example\n",
    "    @param p_yneg, float, probability of encountering a negative example\n",
    "    returns np.float64\n",
    "    \"\"\"\n",
    "    #your code goes here ↓↓↓\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code2.1.4"
    ]
   },
   "outputs": [],
   "source": [
    "def calc_func_g(par_A,par_b,par_c,gridpoints):\n",
    "    \"\"\"\n",
    "    Combine the previously calculated parameters to the optimal classification function g.\n",
    "    Return in shape [500,500]. The 500x500 grid will plot nicely later.\n",
    "    Avoid hardcoding, i.e. use int(np.sqrt(gridpoints.shape[0]) instead of the number 500\n",
    "    @param gridpoints, np.array, the points the function g should be applied to \n",
    "    returns np.ndarray of shape (500,500)\n",
    "    \"\"\"\n",
    "    #your code goes here ↓↓↓\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "\n",
    "X1, X2 = np.mgrid[-11:11:500j, -11:11:500j]\n",
    "gridpoints = np.c_[X1.ravel(), X2.ravel()]\n",
    "    \n",
    "par_A = calc_par_A(meanXpos,covXpos,meanXneg,covXneg,p_ypos,p_yneg)\n",
    "par_b = calc_par_b(meanXpos,covXpos,meanXneg,covXneg,p_ypos,p_yneg)\n",
    "par_c = calc_par_c(meanXpos,covXpos,meanXneg,covXneg,p_ypos,p_yneg)\n",
    "func_g = calc_func_g(par_A, par_b,par_c,gridpoints)\n",
    "print(\"gridponts.shape =\",gridpoints.shape,\"\\n\")\n",
    "print(\"func_g.shape =\",func_g.shape,\"\\n\")\n",
    "print(\"A = \",par_A)\n",
    "print(\"A.shape = \",par_A.shape,\"\\n\")\n",
    "print(\"b = \",par_b)\n",
    "print(\"b.shape = \",par_b.shape,\"\\n\")\n",
    "print(\"c = \",par_c)\n",
    "print(\"c.shape = \",par_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">2.2 Code & question (10 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code2.2"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualize the data and the classifier with a scatter plot\n",
    "def scatter_plot2(X,X1,X2,y,g):\n",
    "    \"\"\"Creates a scatter-plot for the dataset X with labels y and the classification function g\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        data\n",
    "    X1: np.ndarray\n",
    "        grid x values\n",
    "    X2: np.ndarray\n",
    "        grid y values\n",
    "    y : np.ndarray\n",
    "        labels\n",
    "    g : np.ndarray\n",
    "        the matrix from your Gaussian classifier\n",
    "    Returns\n",
    "    -------\n",
    "    Figure\n",
    "        a matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig2 = plt.figure(figsize=(8,5))\n",
    "    # your code goes here ↓↓↓\n",
    "    \n",
    "    return fig2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "g = calc_func_g(par_A,par_b,par_c,gridpoints)\n",
    "fig = scatter_plot2(X, X1, X2, y, g)\n",
    "assert isinstance(fig, Figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "***Answer the following questions about the plot you just created:***\n",
    "\n",
    "a22_) Did the classifier perform well on the task i.e. do the decision boundaries seem to match the classes as plotted in Task 1.1?<br>\n",
    "b22_) Are datapoints that lie in the middle (i.e. overlapping) region of the two classes less prone to being misclassified compared to data far away from the center?<br>\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question.<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "q2.2"
    ]
   },
   "outputs": [],
   "source": [
    "# examples for you\n",
    "example_of_true_variable = True\n",
    "example_of_false_variable = False\n",
    "\n",
    "# your answers go here ↓↓↓\n",
    "a22_ = None\n",
    "b22_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 3: Details for bias-variance decomposition for quadratic loss (15 points)</h2>\n",
    "\n",
    "An explicit formula of the bias variance decomposition for the quadratic loss was mentioned in the lecture. In this task, you will prove this decomposition yourselves. To this end, let us introduce some notation:\n",
    "\n",
    "$Z_l = (X,\\mathbf{y})$ denotes a data matrix of $l$ elements, with $X$ the ($d\\times l$)-dimensional feature matrix and $\\mathbf{y}$ the $l$-dimensional (column) label vector. $g(\\mathbf{x}_0;\\mathbf{w}(Z_l)))$ denotes the model, parametrized by the vector $\\mathbf{w}(Z_l)$ trained on $Z_l$, and the variable $y$ is the label corresponding to a (new) feature vector $\\mathbf{x}_0$. \n",
    "\n",
    "Our object of interest is the expected prediction error (EPE) for\n",
    "$\\mathbf{x}_0$ in case of the quadratic loss, i.e.:\n",
    "\n",
    "$$\\mathrm{EPE}(\\mathbf{x}_0) = \\mathrm{E}_{y\\mid\n",
    "\\mathbf{x}_0,Z_l}\\big(L_{\\mathbf{q}}(y,g(\\mathbf{x}_0;\\mathbf{w}(Z_l)))\\big)\n",
    "= \\mathrm{E}_{y\\mid\n",
    "\\mathbf{x}_0,Z_l}\\big((y-g(\\mathbf{x}_0;\\mathbf{w}(Z_l)))^2\\big)$$\n",
    "\n",
    "We assume that $y\\!\\mid\\!\\mathbf{x}_0$ and the selection of training samples $Z_l$ are\n",
    "independent which results in the following reformulation of the total expected prediction error:\n",
    "\n",
    "$$\\mathrm{EPE}(\\mathbf{x}_0) = \\mathrm{E}_{y\\mid\n",
    "\\mathbf{x}_0}\\Big(\\mathrm{E}_{Z_l}\\big((y-g(\\mathbf{x}_0;\\mathbf{w}(Z_l)))^2\\big)\\Big) \\qquad \\text{(1)}$$\n",
    "\n",
    "Show that we can obtain the following bias-variance decomposition:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{EPE}(\\mathbf{x}_0)=&\\,\\operatorname{Var}(y\\!\\mid\\!\\mathbf{x}_0) \\\\\n",
    "&+\\Big(\\mathrm{E}_{y\\mid\\mathbf{x}_0}(y)-E_{Z_l}\\big(g(\\mathbf{x}_0;\\mathbf{w}(Z_l))\\big)\\Big)^2 & \\text{(2)}\\\\\n",
    "&+\\mathrm{E}_{Z_l}\\Big(\\big(g(\\mathbf{x}_0;\\mathbf{w}(Z_l))-E_{Z_l}(g(\\mathbf{x}_0;\\mathbf{w}(Z_l)))\\big)^2\\Big) \n",
    "\\end{align}\n",
    "\n",
    "For your calculation please use the given notation. Follow the steps indicated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">3.1 Calculation (5 points): Expand the Expected Prediction Error.</h3>\n",
    "\n",
    "Expand $\\mathrm{EPE}(\\mathbf{x}_0)$, i.e. eq. (1) above, and write it as three separate terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "calc3.1"
    ]
   },
   "source": [
    "Your calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">3.2 Calculation (2 points): Rewrite $\\operatorname{Var}(y\\!\\mid\\!\\mathbf{x}_0)$ using expected values. </h3>\n",
    "\n",
    "Write the label variance (unavoidable error), i.e. term 1 in (2), in terms of expectation values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "calc3.2"
    ]
   },
   "source": [
    "Your calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">3.3. Calculation (3 points): Expand the squared bias.</h3>\n",
    "\n",
    "Expand the squared bias, i.e. term 2 in (2), and write it in three separate terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "calc3.3"
    ]
   },
   "source": [
    "Your calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">3.4 Calculation (5 points): Expand the variance of the model.</h3>\n",
    "\n",
    "Expand model variance, i.e. term 3 in (2), into three terms, and then simplify it to only two terms.\n",
    "\n",
    "Eventually, show that adding up your results from 3.2., 3.3., and 3.4. leads to your results from 3.1. That concludes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "calc3.4"
    ]
   },
   "source": [
    "Your calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 4: Bias-variance decomposition for regression (40 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.1 Question (10 points):</h3>\n",
    "\n",
    "Consider the following one-dimensional regression task: inputs $x$ are\n",
    "sampled from the uniform distribution in $[−1, 3] \\subset \\mathbb{R}$ and targets $y$ are given as\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) &= 0.5\\,x^4 + 2\\,x^3 - 8\\,x^2 \\\\\n",
    "y &= f(x) + \\varepsilon,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\varepsilon$ is independent normally distributed noise with mean $\\mu=0$ and variance $\\sigma^2 = 4$. \n",
    "\n",
    "***What are $E(y\\!\\mid\\!x_0)$ and the unavoidable error $\\operatorname{Var}(y\\!\\mid\\!x_0)$ for a fixed $x_0$ in this setting?***\n",
    "\n",
    "a41_)   $E(y\\!\\mid\\!x_0) = 0.5\\,\\sigma^4 + 2\\,\\sigma^3 - 8\\,\\sigma^2 \\,\\text{  and  } \\operatorname{Var}(y\\!\\mid\\!x_0) = x_0^2$. <br>\n",
    "b41_)   $E(y\\!\\mid\\!x_0) = 0.5\\,\\sigma^4 + 2\\,\\sigma^3 - 8\\,\\sigma^2 \\,\\text{  and  }  \\operatorname{Var}(y\\!\\mid\\!x_0) = \\sigma^2$. <br>\n",
    "c41_)   $E(y\\!\\mid\\!x_0) = 0.5\\,\\sigma^4 + 2\\,\\sigma^3 - 8\\,\\sigma^2 \\,\\text{  and  }  \\operatorname{Var}(y\\!\\mid\\!x_0) = 2\\sigma^2$. <br>\n",
    "d41_)   $E(y\\!\\mid\\!x_0) = 0.5\\,x_0^4 + 2\\,x_0^3 - 8\\,x_0^2 \\,\\text{  and  }\\operatorname{Var}(y\\!\\mid\\!x_0) = 2\\sigma^2$. <br>\n",
    "e41_)   $E(y\\!\\mid\\!x_0) = 0.5\\,x_0^4 + 2\\,x_0^3 - 8\\,x_0^2 \\,\\text{  and  }\\operatorname{Var}(y\\!\\mid\\!x_0) = \\sigma^2$. <br>\n",
    "f41_)   $E(y\\!\\mid\\!x_0) = 0.5\\,x_0^4 + 2\\,x_0^3 - 8\\,x_0^2 \\,\\text{  and  } \\operatorname{Var}(y\\!\\mid\\!x_0) = 0.5\\,x_0^4 + 2\\,x_0^3 - 8\\,x_0^2+\\sigma^2$.<br>\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative 10/3 points and no answer (i.e. answer “None”) gives 0 points for a question. The correct answer will give 10 points. You cannot fall below 0 points in total.<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "q4.1"
    ]
   },
   "outputs": [],
   "source": [
    "# examples for you\n",
    "example_of_true_variable = True\n",
    "example_of_false_variable = False\n",
    "\n",
    "# your answers go here ↓↓↓\n",
    "a41_=None\n",
    "b41_=None\n",
    "c41_=None\n",
    "d41_=None\n",
    "e41_=None\n",
    "f41_=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We intend to perform polynomial regression to illustrate the bias-variance decomposition for the regression task described before. To this end, perform the following steps:\n",
    " * **Task 4.2**:\n",
    "    * Define the function `func_f`, using the definition in 4.1.\n",
    "    * Implement the function `create_train_X` which should return $k=300$ training sets with $l=25$ samples in the form of a numpy array. The x-values are sampled from xmin to xmax (later we shall use the specified intervall: $[-1,3]$).\n",
    "    * Implement the function `create_train_y` which generated the training y-values according to 4.1, i.e. with Gaussian noise. Define the function for general mean mu and standard deviation std (later we shall use the specified values).\n",
    "    * Below, we provide the code for a function `pol_reg_pred` that trains a polynomial regression model with degree $m$ on a given training set and returns the prediction for a given test set (uniformly sampled $x$ values without labels). Use this to implement the function `bias_var` that estimates for each degree $m=1,...,11$ the squared bias and the variance from the predictions for each of the $k=300$ training sets at $x_0=1.7$ and stores them in the lists sqbias and variance (which are already initiated as empty lists). Each of these two lists should then only contain $11$ elements.\n",
    " * **Task 4.3**: \n",
    "   * Utilize the function `pol_reg_pred` to produce a <em>single</em> plot that simultaneously visualizes the training data as dots (plot only the <em>first</em> instance of the $k$ training sets, i.e. the 25 points from the first set) and the corresponding models for $m=1,3,11$. Don't forget to label the axes. Note: Make sure to produce the plot in the correct (second) cell.\n",
    "   * Finally, visualize your results in another <em>single</em> plot where the dependence of (i) the unavoidable error, (ii) the squared bias, (iii) the model variance, and (iv) the total EPE is shown versus $m$. Again, label the axis and plot a legend. Feel free to plot lines to guide the eye, although the horizontal axis is discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.2 Code (15 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "imports2"
    ]
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# do not change the seed\n",
    "np.random.seed(12)\n",
    "\n",
    "def pol_reg_pred(X_train,y_train,X_test,m):\n",
    "    \"\"\"\n",
    "    Function that trains a polynomial regression model with degree on a given training set \n",
    "    and returns the prediction for a given test set (uniformly sampled values without labels).\n",
    "    @param X_train, np.ndarray, training samples\n",
    "    @param y_train, np.ndarray, training labels\n",
    "    @param X_test, np.ndarray, test samples\n",
    "    @param m, int, degree of polynomial\n",
    "    \"\"\"\n",
    "    np.random.seed(12)\n",
    "    poly_reg = PolynomialFeatures(m)\n",
    "    X_poly_train = poly_reg.fit_transform(X_train.reshape(-1, 1))\n",
    "    X_poly_test= poly_reg.fit_transform(X_test.reshape(-1, 1))\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_poly_train, y_train)\n",
    "    y_pred = lin_reg.predict(X_poly_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code4.2.1"
    ]
   },
   "outputs": [],
   "source": [
    "def func_f(x):\n",
    "    \"\"\"\n",
    "    Implementation of the polynomial from 4.1\n",
    "    @param x, value you pass to the function\n",
    "    \"\"\"\n",
    "    # your code goes here ↓↓↓\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code4.2.2"
    ]
   },
   "outputs": [],
   "source": [
    "def create_train_X(k,l,xmin,xmax):\n",
    "    \"\"\"\n",
    "    Function that creates k training sets with l samples\n",
    "    @param k, number of training sets to create\n",
    "    @param l, number of samples per training set\n",
    "    @param xmin, lower bound of sample interval\n",
    "    @param xmax, upper bound of sample interval\n",
    "    \"\"\"\n",
    "    # use the numpy.random module and dont change the seed!\n",
    "    np.random.seed(12)\n",
    "    \n",
    "    # your code goes here ↓↓↓\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code4.2.3"
    ]
   },
   "outputs": [],
   "source": [
    "def create_train_y(k,l,X_train,mu,std,func):\n",
    "    \"\"\"\n",
    "    Function that creates labels from training data with func_f and gaussian noise \n",
    "    @param k, number of label sets to create\n",
    "    @param l, number of labels per set\n",
    "    @param X_train, training set\n",
    "    @param mu, mean of gaussian\n",
    "    @param std, std of gaussian\n",
    "    @param func, callable, polynomial function\n",
    "    returns np.ndarray \n",
    "    \"\"\"\n",
    "    # use the numpy.random module and dont change the seed!\n",
    "    np.random.seed(12)\n",
    "    \n",
    "    # your code goes here ↓↓↓\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code4.2.4"
    ]
   },
   "outputs": [],
   "source": [
    "def bias_var(X_train,y_train,M,k,func): \n",
    "    \"\"\"\n",
    "    Function that computes model bias and variance \n",
    "    @param X_train, np.ndarray, training data\n",
    "    @param y_train, np.ndarray, training labels\n",
    "    @param M, int, upper bound on m (degree of the polynomial)\n",
    "    @param k, int, number of sample sets\n",
    "    @param func, callable, polynomial function\n",
    "    returns sqbias,variance \n",
    "    \"\"\"\n",
    "    x0 = np.array([1.7])\n",
    "    sqbias = []\n",
    "    variance = []\n",
    "    \n",
    "    # your code goes here ↓↓↓\n",
    "\n",
    "    return (sqbias,variance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## If you get deprecation warnings from numpy in the following cell, uncomment these two lines:\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "## Else: Nothing to do here, just run the cell.\n",
    "k = 300\n",
    "l = 25\n",
    "M = 11\n",
    "xmin=-1\n",
    "xmax=3\n",
    "mu=0\n",
    "sigmasq=4\n",
    "std=np.sqrt(sigmasq)\n",
    "\n",
    "X_train = create_train_X(k,l,xmin,xmax)\n",
    "y_train = create_train_y(k,l,X_train,mu,std,func_f)\n",
    "sqbias, variance = bias_var(X_train,y_train,M,k,func_f)\n",
    "\n",
    "print(\"Shapes of X and y: \\n\",X_train.shape,y_train.shape)\n",
    "print(\"\\nSquared Bias over m: \\n\", sqbias)\n",
    "print(\"\\nVariance over m: \\n\", variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.3 Code (10 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "data2"
    ]
   },
   "outputs": [],
   "source": [
    "## test data, from -1 to 3 in steps of 0.01\n",
    "# Nothing to do here, just run the cell.\n",
    "np.random.seed(12)\n",
    "x_ = np.arange(xmin, xmax, 0.01)   ## test data, from -1 to 3 in steps of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code4.3.1"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualize the datapoints and the classifiers with different m = [1,3,11]\n",
    "def plot3(X_train, y_train, x, pol_reg_):\n",
    "    \"\"\"creates a plot for the training data and the corresponding regression models with different m\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        training data\n",
    "    y_train : np.ndarray\n",
    "        labels\n",
    "    x       : np.ndarray\n",
    "        test data\n",
    "    pol_reg_ : function\n",
    "        polynomial regression function\n",
    "    Returns\n",
    "    -------\n",
    "    Figure\n",
    "        a matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig3 = plt.figure(figsize=(8,5))\n",
    "    # your code goes here ↓↓↓\n",
    "    \n",
    "    return fig3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "fig3 = plot3(X_train, y_train,x_, pol_reg_pred)\n",
    "assert isinstance(fig, Figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code4.3.2"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualize unavoidable error, squared bias, model variance and total EPE vs. m in [1,11]\n",
    "def plot4(M, sigmasq, biassq, var):\n",
    "    \"\"\"creates a plot for  unavoidable error, bias, variance and EPE vs. m in [1,11]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M    : int\n",
    "        m is in [1,M]\n",
    "    sigmasq: float\n",
    "        unavoidable error\n",
    "    biassq : np.ndarray\n",
    "        squared bias\n",
    "    var  : np.ndarray\n",
    "        model variance \n",
    "    Returns\n",
    "    -------\n",
    "    Figure\n",
    "        a matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig4 = plt.figure(figsize=(8,5))\n",
    "    # your code goes here ↓↓↓\n",
    "    \n",
    "    return fig4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "fig4 = plot4(M, sigmasq, sqbias, variance)\n",
    "assert isinstance(fig, Figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.4 Question (5 points):</h3>\n",
    "\n",
    "If you did the previous task correctly, the resulting plot should look similar to the following:\n",
    "\n",
    "![bias-variance.png](bias-variance.png)\n",
    "\n",
    "\n",
    "***What observations can you make from this plot? Tick the correct boxes (several may be correct):***\n",
    "\n",
    "a42_) The variance is low for models which are too complex, i.e. $m \\leq 2$. <br>\n",
    "b42_) For appropriate complexity, i.e. $3 \\leq m \\leq 6$, both model variance and bias are low, which indicates good generalization abilities.<br>\n",
    "c42_) As the model becomes too complex, i.e. $m \\geq 9$, the variance increases again while the bias still decreases. This is an indication for overfitting.<br>\n",
    "d42_) For models with $m \\geq 9$, the variance is high (i.e. significantly larger than $0$) because the independent noise has zero mean and high individual biases cancel in expectation. <br>\n",
    "e42_) For models with $m \\geq 9$, the bias is still low (i.e. close to $0$) because the independent noise has zero mean and high individual biases cancel in expectation. <br>\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question.<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "q4.4"
    ]
   },
   "outputs": [],
   "source": [
    "# examples for you\n",
    "example_of_true_variable = True\n",
    "example_of_false_variable = False\n",
    "\n",
    "# your answers go here ↓↓↓\n",
    "a42_=None\n",
    "b42_=None\n",
    "c42_=None\n",
    "d42_=None\n",
    "e42_=None\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 5: Evaluation metrics for imbalanced data sets (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Consider a classifier with discriminant function $\\bar g$.\n",
    "For a given labeled data set, the following results are obtained:\n",
    "$$\\begin{array}{|r|r|}\n",
    "\\hline\n",
    "y & \\bar g(x)\\\\\n",
    "\\hline \\hline\n",
    " +1 & 0.93 \\\\\n",
    " +1 & 0.51 \\\\\n",
    " +1 & 0.48 \\\\\n",
    " -1 & 0.13 \\\\\n",
    " +1 & 0.02 \\\\\n",
    " -1 & -0.11 \\\\\n",
    " -1 & -0.25 \\\\\n",
    " -1 & -0.37 \\\\\n",
    " +1 & -0.41 \\\\\n",
    " -1 & -1.68 \\\\\n",
    " +1 & -2.23 \\\\\n",
    " -1 & -3.41 \\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "* **Task 5.1**:\n",
    "  * Compute the confusion matrix using the usual $\\theta = 0$ threshold.\n",
    "  * Complete the function evaluation_measures to calculate the following evaluation measures: TPR, TNR, FPR, FNR, ACC, BACC, PREC, and $F_1$, and store the exact results in the respective variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">5.1 Calculation (8 points):</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* <b>Reminder:</b> Confusion Matrix structure:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & g(x)=+1  & g(x)=-1\\\\\n",
    " \\hline\n",
    " y=+1 & \\text{TP}  & \\text{FN} \\\\\n",
    " \\hline\n",
    " y=-1 & \\text{FP} & \\text{TN} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "q5.1"
    ]
   },
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "# your answers (integers!) go here ↓↓↓ \n",
    "_TP = 0\n",
    "_FN = 0\n",
    "_FP = 0\n",
    "_TN = 0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code5.1"
    ]
   },
   "outputs": [],
   "source": [
    "# evaluation measures\n",
    "def evaluation_measures(TP,TN,FP,FN):\n",
    "    \"\"\"\n",
    "    This function should return the desired evaluation measures. \n",
    "    @param TP, True Positives\n",
    "    @param FN, False Negatives\n",
    "    @param FP, False Positives\n",
    "    @param TN, True Negatives\n",
    "    returns TPR,TNR,FPR,FNR,ACC,BACC,PREC,F1\n",
    "    \"\"\"    \n",
    "    # your code goes here ↓↓↓\n",
    "\n",
    "    return (TPR,TNR,FPR,FNR,ACC,BACC,PREC,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_TPR,_TNR,_FPR,_FNR,_ACC,_BACC,_PREC,_F1 = evaluation_measures(_TP,_TN,_FP,_FN)\n",
    "\n",
    "print(\" TPR = {:.3f}\\n TNR = {:.3f}\\n FPR = {:.3f}\\n FNR = {:.3f}\\n ACC = {:.3f}\\nBACC = {:.3f}\\nPREC = {:.3f}\\n  F1 = {:.3f}\".format(_TPR,_TNR,_FPR,_FNR,_ACC,_BACC,_PREC,_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's say we have a population of 1000 people and we know that 50 are infected with the corona virus.\n",
    "* **Task 5.2**: Assume that the population is tested with an assay that has a certain specificity and sensitivity.\n",
    "\n",
    "  * (1) What is the probability $p_1$ that a person is *not* infected if they are diagnosed as ill by the test? <br>\n",
    "  * (2) What is the probability $p_2$ that a person is infected if they are diagnosed as healthy by the test?\n",
    "\n",
    "Write a function that returns both values. Then check your calculation using specificity of $90 \\%$ and sensitivity of $95 \\%$.\n",
    "\n",
    "**Note**: Round your result to 4 decimal points, i.e. 0.9871 if it is 98.71%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">5.2 Calculation (7 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code5.2"
    ]
   },
   "outputs": [],
   "source": [
    "def calc_prob(spec,sens,pop,inf):\n",
    "    \"\"\"\n",
    "    This function should return the desired probabilities p1 and p2. \n",
    "    @param spec, float, specificity\n",
    "    @param sens, float, sensitivity\n",
    "    @param pop, int, population\n",
    "    @param inf, int, infected\n",
    "    returns p1,p2\n",
    "    \"\"\"\n",
    "    population = pop\n",
    "    infected = inf\n",
    "    # your code goes here ↓↓↓\n",
    "\n",
    "    return p1,p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_result = calc_prob(0.90,0.95,1000,50)\n",
    "print(\"The probability that a person who is tested positive is in fact not infected is {}\".format(_result[0]))\n",
    "print(\"The probability that a person who is tested negative is in fact infected is {}\".format(_result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "exec"
    ]
   },
   "outputs": [],
   "source": [
    "# executability check\n",
    "est_mean_cov(X,y)\n",
    "calc_par_A(np.ones(shape=2),np.eye(2),np.ones(shape=2),np.eye(2),1,1)\n",
    "calc_par_b(np.ones(shape=2),np.eye(2),np.ones(shape=2),np.eye(2),1,1)\n",
    "calc_par_c(np.ones(shape=2),np.eye(2),np.ones(shape=2),np.eye(2),1,1)\n",
    "calc_func_g(np.ones(shape=(2,2)),np.ones(2),1,np.ones(shape=(250000,2)))\n",
    "create_train_X(1,1,1,2)\n",
    "create_train_y(1,1,np.ones((300,25)),1,1,func_f)\n",
    "bias_var(np.ones((300,25)),np.ones((300,25)),M,k,func_f)\n",
    "evaluation_measures(1,1,1,1)\n",
    "calc_prob(1,1,2,1)\n",
    "print(\"Executable\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "356734b0b4e05b3af569ed06eb258f6ef66038e7268c6bdbb97ecd1a1c609e88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
